{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7649273,"sourceType":"datasetVersion","datasetId":4459076}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchinfo","metadata":{"id":"9xwPMJJ5KIVe","trusted":true,"execution":{"iopub.status.busy":"2025-08-09T14:26:51.196306Z","iopub.execute_input":"2025-08-09T14:26:51.196538Z","iopub.status.idle":"2025-08-09T14:26:55.115479Z","shell.execute_reply.started":"2025-08-09T14:26:51.196514Z","shell.execute_reply":"2025-08-09T14:26:55.114553Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (1.8.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import time\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import classification_report\nimport numpy as np\nimport cv2\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nfrom torchvision import transforms\nfrom torchinfo import summary\nimport torchvision.models as models\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import accuracy_score\nimport PIL\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nfrom collections import OrderedDict\nimport platform\nimport psutil\nimport random\nimport glob\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom torchvision import transforms\nfrom torchvision.transforms import ColorJitter, RandomRotation, RandomResizedCrop\nfrom torchvision.transforms.functional import gaussian_blur\nfrom PIL import ImageOps\nfrom tabulate import tabulate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T14:26:55.116500Z","iopub.execute_input":"2025-08-09T14:26:55.116789Z","iopub.status.idle":"2025-08-09T14:27:03.249913Z","shell.execute_reply.started":"2025-08-09T14:26:55.116759Z","shell.execute_reply":"2025-08-09T14:27:03.249158Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"cpu_info = platform.processor()\n\nram_info = psutil.virtual_memory()\ntotal_ram_gb = ram_info.total / (1024 ** 3)\n\ntry:\n    gpu_info = !nvidia-smi --query-gpu=gpu_name --format=csv\n    gpu_name = gpu_info[1]\nexcept:\n    gpu_name = \"No GPU available\"\n\nprint(\"CPU:\", cpu_info)\nprint(\"Total RAM (GB):\", round(total_ram_gb, 2))\nprint(\"GPU:\", gpu_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T14:27:03.251410Z","iopub.execute_input":"2025-08-09T14:27:03.251730Z","iopub.status.idle":"2025-08-09T14:27:03.282847Z","shell.execute_reply.started":"2025-08-09T14:27:03.251713Z","shell.execute_reply":"2025-08-09T14:27:03.282308Z"}},"outputs":[{"name":"stdout","text":"CPU: x86_64\nTotal RAM (GB): 31.35\nGPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"seed = 1\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T14:27:03.283525Z","iopub.execute_input":"2025-08-09T14:27:03.283813Z","iopub.status.idle":"2025-08-09T14:27:03.292471Z","shell.execute_reply.started":"2025-08-09T14:27:03.283795Z","shell.execute_reply":"2025-08-09T14:27:03.291780Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"Dataset = \"/kaggle/input/melanoma-cancer-dataset\"\n\nfile_paths = []\nlabels = []\n\nfor class_name in os.listdir(Dataset):\n    class_dir = os.path.join(Dataset, class_name)\n    for image_name in os.listdir(class_dir):\n        file_paths.append(os.path.join(class_dir, image_name))\n        labels.append(class_name)\n\ndf = pd.DataFrame({\"file_path\": file_paths, \"label\": labels})\ndf = df.sample(frac=1).reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-09T14:27:03.293152Z","iopub.execute_input":"2025-08-09T14:27:03.293684Z","iopub.status.idle":"2025-08-09T14:27:03.315144Z","shell.execute_reply.started":"2025-08-09T14:27:03.293657Z","shell.execute_reply":"2025-08-09T14:27:03.314465Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torchvision.models as models\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as F\nimport torchvision.utils as vutils\n\nfrom torchvision.datasets import CIFAR10\n\nfrom torch.utils.data import DataLoader, Dataset\n\nimport numpy as np\nimport os\nfrom tqdm import tqdm\nfrom PIL import Image","metadata":{"id":"y4ZAkuftK0xc","trusted":true,"execution":{"iopub.status.busy":"2025-08-09T14:27:03.315871Z","iopub.execute_input":"2025-08-09T14:27:03.316056Z","iopub.status.idle":"2025-08-09T14:27:03.320206Z","shell.execute_reply.started":"2025-08-09T14:27:03.316041Z","shell.execute_reply":"2025-08-09T14:27:03.319623Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class ContrastiveDataset(Dataset):\n    def __init__(self, dataset, transform):\n        self.dataset = dataset\n        self.transform = transform\n\n    def __getitem__(self, index):\n        img, _ = self.dataset[index]\n        img1 = self.transform(img)\n        img2 = self.transform(img)\n        return img1, img2\n\n    def __len__(self):\n        return len(self.dataset)","metadata":{"id":"DlG2E-g0K2oz","trusted":true,"execution":{"iopub.status.busy":"2025-08-09T14:27:03.320753Z","iopub.execute_input":"2025-08-09T14:27:03.320948Z","iopub.status.idle":"2025-08-09T14:27:03.334306Z","shell.execute_reply.started":"2025-08-09T14:27:03.320933Z","shell.execute_reply":"2025-08-09T14:27:03.333678Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"transform = T.Compose([\n    T.RandomResizedCrop(size=32),\n    T.RandomHorizontalFlip(),\n    T.ColorJitter(0.5, 0.5, 0.5, 0.5),\n    T.RandomGrayscale(p=0.2),\n    T.ToTensor()\n])","metadata":{"id":"lKxMakVsK4m7","trusted":true,"execution":{"iopub.status.busy":"2025-08-09T14:27:03.336102Z","iopub.execute_input":"2025-08-09T14:27:03.336561Z","iopub.status.idle":"2025-08-09T14:27:03.346216Z","shell.execute_reply.started":"2025-08-09T14:27:03.336543Z","shell.execute_reply":"2025-08-09T14:27:03.345373Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_dataset = CIFAR10(root='./data', download=True, train=True)\ncontrastive_dataset = ContrastiveDataset(train_dataset, transform)\ntrain_loader = DataLoader(contrastive_dataset, batch_size=256, shuffle=True)","metadata":{"id":"RLp-Mq2ZK7AT","trusted":true,"execution":{"iopub.status.busy":"2025-08-09T14:27:03.346989Z","iopub.execute_input":"2025-08-09T14:27:03.347406Z","iopub.status.idle":"2025-08-09T14:27:18.954444Z","shell.execute_reply.started":"2025-08-09T14:27:03.347373Z","shell.execute_reply":"2025-08-09T14:27:18.953829Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 170M/170M [00:11<00:00, 14.8MB/s] \n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"class SimCLR(nn.Module):\n    def __init__(self, base_model, projection_dim=128):\n        super(SimCLR, self).__init__()\n        self.encoder = base_model\n        self.encoder.fc = nn.Identity()  # remove original fc\n        self.projection_head = nn.Sequential(\n            nn.Linear(2048, 512),\n            nn.ReLU(),\n            nn.Linear(512, projection_dim)\n        )\n\n    def forward(self, x):\n        features = self.encoder(x)\n        projections = self.projection_head(features)\n        return projections","metadata":{"id":"6jmWJ9sZLIFU","trusted":true,"execution":{"iopub.status.busy":"2025-08-09T14:27:18.955170Z","iopub.execute_input":"2025-08-09T14:27:18.955399Z","iopub.status.idle":"2025-08-09T14:27:18.959975Z","shell.execute_reply.started":"2025-08-09T14:27:18.955382Z","shell.execute_reply":"2025-08-09T14:27:18.959297Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"model = SimCLR(models.resnext101_32x8d(pretrained=False))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GqNvGRlyLJ8V","outputId":"29743458-1503-4b33-8f42-65e472c47075","trusted":true,"execution":{"iopub.status.busy":"2025-08-09T14:27:18.960932Z","iopub.execute_input":"2025-08-09T14:27:18.961206Z","iopub.status.idle":"2025-08-09T14:27:20.399979Z","shell.execute_reply.started":"2025-08-09T14:27:18.961178Z","shell.execute_reply":"2025-08-09T14:27:20.399366Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def nt_xent_loss(z_i, z_j, temperature=0.5):\n    batch_size = z_i.size(0)\n    z = torch.cat([z_i, z_j], dim=0)\n    z = F.normalize(z, dim=1)\n\n    similarity = torch.matmul(z, z.T) / temperature\n    # create labels for each sample\n    labels = torch.arange(batch_size).to(z.device)\n    labels = torch.cat([labels, labels], dim=0)\n\n    # mask out the diagonal elements (similarity with itself)\n    mask = torch.eye(batch_size * 2, dtype=torch.bool).to(z.device)\n    similarity = similarity.masked_fill(mask, -9e15)\n\n    # get the similarity between positive pairs\n    positives = torch.diag(similarity, batch_size) + torch.diag(similarity, -batch_size)\n    numerator = torch.exp(positives)\n\n    # calculate the denominator by summing the exponentiated similarity scores for all samples\n    # except the positive pair for each sample\n    # mask out the positive pairs\n    mask_positives = torch.eye(batch_size * 2, dtype=torch.bool).to(z.device)\n    for i in range(batch_size):\n        mask_positives[i, batch_size + i] = True\n        mask_positives[batch_size + i, i] = True\n    # get the denominator by summing the exponentiated similarity scores for all samples\n    # except the positive pair for each sample\n    denominator = torch.exp(similarity).masked_fill(mask_positives, 0).sum(dim=1)\n     # Fix: Repeat numerator to match denominator size\n    numerator = numerator.repeat(2)  # Now numerator shape is (512,)\n    # remove the zeros in denominator\n    denominator = denominator + numerator\n    # calculate the loss\n    loss = -torch.log(numerator / denominator).mean()\n\n    return loss","metadata":{"id":"exxRd-1u9Npm","trusted":true,"execution":{"iopub.status.busy":"2025-08-09T14:27:20.400683Z","iopub.execute_input":"2025-08-09T14:27:20.400903Z","iopub.status.idle":"2025-08-09T14:27:20.407567Z","shell.execute_reply.started":"2025-08-09T14:27:20.400887Z","shell.execute_reply":"2025-08-09T14:27:20.407003Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Device: {device}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WF4DfRWqFBJM","outputId":"c6768255-a7f1-4a82-8d35-8d2c28285b4a","trusted":true,"execution":{"iopub.status.busy":"2025-08-09T14:27:20.408295Z","iopub.execute_input":"2025-08-09T14:27:20.408535Z","iopub.status.idle":"2025-08-09T14:27:20.494690Z","shell.execute_reply.started":"2025-08-09T14:27:20.408514Z","shell.execute_reply":"2025-08-09T14:27:20.493787Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom tqdm import tqdm\n\n# Assume model, train_loader, nt_xent_loss, device are defined properly\n\nmodel = model.to(device)\noptimizer = optim.Adam(model.parameters(), lr=3e-4)\nsave_dir = '/kaggle/working'  # Save locally in Kaggle environment\n\npretext_losses = []\n\nfor epoch in range(100):  # 100 epochs\n    model.train()\n    total_loss = 0\n    for img1, img2 in tqdm(train_loader):\n        img1, img2 = img1.to(device), img2.to(device)\n        z1 = model(img1)\n        z2 = model(img2)\n        \n        # Normalize embeddings before loss\n        z = torch.cat([z1, z2], dim=0)\n        z = F.normalize(z, p=2, dim=1)\n        \n        # Compute NT-Xent loss (make sure your loss uses normalized z1, z2 or z)\n        loss = nt_xent_loss(z1, z2)  # Or update inside nt_xent_loss to normalize if needed\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    \n    avg_loss = total_loss / len(train_loader)\n    pretext_losses.append(avg_loss)\n    print(f\"Epoch [{epoch+1}/10] Pretext Loss: {avg_loss:.4f}\")\n    \n    # Save model checkpoint after each epoch\n    model_path = os.path.join(save_dir, 'simclr_pretask_resnext101_32x8d.pth')\n    torch.save(model.state_dict(), model_path)\n    print(f\"Model saved to {model_path}\")\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ImHtLy9ELTJU","outputId":"8eb64b56-9bdc-4211-8c5b-a9822d8897c0","trusted":true,"execution":{"iopub.status.busy":"2025-08-09T14:27:20.495564Z","iopub.execute_input":"2025-08-09T14:27:20.495913Z","execution_failed":"2025-08-09T16:51:22.981Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 196/196 [05:56<00:00,  1.82s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10] Pretext Loss: 4.2497\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:56<00:00,  1.82s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/10] Pretext Loss: 4.2431\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:55<00:00,  1.82s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:54<00:00,  1.81s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:54<00:00,  1.81s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:55<00:00,  1.81s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:53<00:00,  1.80s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:56<00:00,  1.82s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:56<00:00,  1.82s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:56<00:00,  1.82s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:55<00:00,  1.81s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [11/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:55<00:00,  1.82s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [12/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:55<00:00,  1.81s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [13/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:54<00:00,  1.81s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [14/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:53<00:00,  1.81s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [15/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:53<00:00,  1.80s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [16/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:53<00:00,  1.80s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [17/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:55<00:00,  1.81s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [18/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:54<00:00,  1.81s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [19/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:55<00:00,  1.81s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [20/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:59<00:00,  1.84s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [21/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:57<00:00,  1.82s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [22/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:55<00:00,  1.82s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [23/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 196/196 [05:55<00:00,  1.81s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [24/10] Pretext Loss: 4.2430\nModel saved to /kaggle/working/simclr_pretask_resnext101_32x8d.pth\n","output_type":"stream"},{"name":"stderr","text":" 26%|██▌       | 50/196 [01:30<04:24,  1.82s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"save_dir = '/kaggle/working'\nmodel_path = os.path.join(save_dir, 'simclr_classification.pth')\n\ntorch.save(model.state_dict(), model_path)\n\nmodel.load_state_dict(torch.load(model_path, map_location=device))","metadata":{"id":"nloseOGpFEIb","trusted":true,"execution":{"execution_failed":"2025-08-09T16:51:22.984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nimport torchvision.transforms as T\nfrom tqdm import tqdm\nimport os\n\n# Assume 'model' is your pretrained SimCLR model with a resnext101_32x8d encoder\n# and 'device' is your CUDA or CPU device\n\n# Freeze encoder weights\nfor param in model.encoder.parameters():\n    param.requires_grad = False\n\n# Fix classifier input size (2048 for resnext101_32x8d encoder output) and output classes=10\nclassifier = nn.Linear(2048, 10).to(device)\n\noptimizer = optim.Adam(classifier.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\ntransform = T.Compose([T.ToTensor()])\nlabeled_dataset = CIFAR10(root='./data', train=True, transform=transform, download=True)\nlabeled_loader = DataLoader(labeled_dataset, batch_size=256, shuffle=True)\n\ndownstream_losses = []\ndownstream_accuracies = []\n\nsave_dir = '/content/drive/MyDrive/SelfSupervise'  # Your save directory\nos.makedirs(save_dir, exist_ok=True)\n\nfor epoch in range(15):\n    classifier.train()\n    total_loss = 0\n    correct, total = 0, 0\n\n    for imgs, labels in tqdm(labeled_loader):\n        imgs, labels = imgs.to(device), labels.to(device)\n        with torch.no_grad():\n            features = model.encoder(imgs)\n\n        logits = classifier(features)\n        loss = criterion(logits, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        preds = logits.argmax(dim=1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    avg_loss = total_loss / len(labeled_loader)\n    accuracy = (correct / total) * 100\n    downstream_losses.append(avg_loss)\n    downstream_accuracies.append(accuracy)\n\n    print(f\"Epoch [{epoch+1}/15] Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n\n    save_path = os.path.join(save_dir, 'simclr_classifier.pth')\n    torch.save(classifier.state_dict(), save_path)\n    print(f\"Classifier weights saved to {save_path}\")\n\n","metadata":{"id":"NSAXRK0BB9g5","trusted":true,"execution":{"execution_failed":"2025-08-09T16:51:22.985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot SimCLR pretext loss\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, len(pretext_losses) + 1), pretext_losses, marker='o', label='Contrastive Loss')\nplt.title('SimCLR Pretext Loss Curve')\nplt.xlabel('Epoch')\nplt.ylabel('NT-Xent Loss')\nplt.legend()\nplt.grid()\nplt.show()\n\n# Plot downstream classifier loss\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, len(downstream_losses) + 1), downstream_losses, marker='o', color='red', label='Classifier Loss')\nplt.title('Downstream Classifier Loss Curve')\nplt.xlabel('Epoch')\nplt.ylabel('Cross-Entropy Loss')\nplt.legend()\nplt.grid()\nplt.show()\n\n# Plot downstream classifier accuracy\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, len(downstream_accuracies) + 1), downstream_accuracies, marker='s', color='green', label='Classifier Accuracy')\nplt.title('Downstream Classifier Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy (%)')\nplt.legend()\nplt.grid()\nplt.show()","metadata":{"id":"8ys9d7pdJdmc","trusted":true,"execution":{"execution_failed":"2025-08-09T16:51:22.986Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score\nfrom torchvision.datasets import CIFAR10\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as T\n\n# Prepare test dataset and dataloader\ntransform = T.ToTensor()\ntest_dataset = CIFAR10(root='./data', train=False, transform=transform, download=True)\ntest_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n\nall_preds, all_labels = [], []\n\nclassifier.eval()\nmodel.encoder.eval()\n\nwith torch.no_grad():\n    for imgs, labels in test_loader:\n        imgs = imgs.to(device)\n        features = model.encoder(imgs)\n        logits = classifier(features)\n        preds = logits.argmax(dim=1).cpu().numpy()\n        all_preds.extend(preds)\n        all_labels.extend(labels.numpy())\n\nprint(classification_report(all_labels, all_preds, target_names=test_dataset.classes))\nprint(f\"Test Accuracy: {accuracy_score(all_labels, all_preds) * 100:.2f}%\")\n","metadata":{"id":"Eg63scdXJkMZ","trusted":true,"execution":{"execution_failed":"2025-08-09T16:51:22.986Z"}},"outputs":[],"execution_count":null}]}